{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9lOBfezfPCX"
   },
   "source": [
    "# From Pixels to Insights: Demystifying Vision Transformers for Image-based Machine Learning ; Hands-on section\n",
    "\n",
    "## Corning - Merck symposium 2023\n",
    "\n",
    "The following sections are covered in the notebook:\n",
    "\n",
    "* Loading benchmark vision datasets\n",
    "* Loading pre-trained Vision Transformer (ViT) models\n",
    "* Fine-tuning a ViT model using new dataset\n",
    "* Understanding the working of attention layers\n",
    "* Understanding the inner components of a vision transformer model\n",
    "\n",
    "- Original vision transformer paper: Alexey Dosovitskiy et al., \"An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n",
    "https://arxiv.org/abs/2010.11929\n",
    "\n",
    "Source for some cells in this notebook: https://colab.research.google.com/github/hirotomusiker/schwert_colab_data_storage/blob/master/notebook/Vision_Transformer_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qzmY5DzcaWE"
   },
   "outputs": [],
   "source": [
    "import torch                                # PyTorch library\n",
    "import torchvision                          # Library from PyTorch project that contains popular vision datasets, \n",
    "                                            # vision models, and image transformations\n",
    "from torch.utils.data import Subset         # Module needed to select a subset of dataset\n",
    "from matplotlib import pyplot as plt        # Image and plot visualization\n",
    "import numpy as np                          # Library to handle arrays and algebraic computations\n",
    "import PIL                                  # Library to handle image I/O\n",
    "from skimage.transform import resize        # Utility to resize input image\n",
    "\n",
    "\n",
    "try:\n",
    "    import timm\n",
    "    from timm import create_model           # PyTorch Image Models (TIMM) is a collection of pre-trained image models and \n",
    "except:                                     # methods required for training ML models\n",
    "    !pip install timm\n",
    "    import timm\n",
    "    from timm import create_model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Ky0N0yAcahh",
    "outputId": "aa92669a-ceff-4fce-f1ce-ff392b536a7e"
   },
   "outputs": [],
   "source": [
    "#---------Define the necessary image transformations--------#\n",
    "\n",
    "transform1 = torchvision.transforms.ToTensor()                   # Method that transforms the image format to PyTorch tensor\n",
    "transform2 = torchvision.transforms.Resize((224,224))            # Method that resizes a RGB image to (224,224,3) \n",
    "\n",
    "transform = torchvision.transforms.Compose([transform1, transform2])   # Compose the two transforms into one master transform.\n",
    "\n",
    "#---------Download the CIFAR 10 dataset-------------------#\n",
    "traindata_full = torchvision.datasets.CIFAR10('./cifar10_root/', download=True, train = True, transform = transform)\n",
    "\n",
    "#---------Overview of the dataset--------------------------#\n",
    "# Number of data\n",
    "num_train_data = len(traindata_full)\n",
    "print('Number of images in the CIFAR10 dataset:', num_train_data)\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(traindata_full.classes)\n",
    "print('Number of classes in the CIFAR10 dataset:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------Manually defining the class labels in the CIFAR10 dataset-------#\n",
    "\n",
    "class_dict = {0: 'plane', 1: 'car', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', \n",
    "              6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Ky0N0yAcahh",
    "outputId": "aa92669a-ceff-4fce-f1ce-ff392b536a7e"
   },
   "outputs": [],
   "source": [
    "#---------------Setting up the dataloader for training----------------#\n",
    "\n",
    "indices = list(range(100))                                    \n",
    "traindata = Subset(traindata_full, indices)                   # Selecting the first 100 images from the whole dataset\n",
    "trainloader = torch.utils.data.DataLoader(traindata, batch_size=4,shuffle=True, num_workers=1)  #batch size = 4\n",
    "\n",
    "\n",
    "#----------------Visualizing images in the dataset--------------#\n",
    "def imshow(img):                                              # Function to visualize image\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))                # Transforming images from (channels, height, width) to (h,w,c)\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(trainloader)                                  # Iterable trainloader\n",
    "\n",
    "for _ in range(3):                                            # Visualizing 3 batches of images (batch size = 4)\n",
    "    print(_)\n",
    "    images, labels = next(dataiter)                           # The 'next' attribute helps to move to the next batch\n",
    "    imshow(torchvision.utils.make_grid(images))               # torchvision method that allows visualizing images in a grid\n",
    "    print([class_dict[label.item()] for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwWcocBdcaq4"
   },
   "source": [
    "### Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------A sneak peek at the overall list of models in timm------------#\n",
    "\n",
    "models = timm.list_models('*', pretrained = True)                             # Select all the models in timm\n",
    "print(\"Random selection of 10 models from the overall list of models:\\n\")\n",
    "print([models[i] for i in torch.randperm(len(models))[:10].tolist()])         # Select 10 random models from the list to display\n",
    "print(\"\\n\")\n",
    "\n",
    "vit_models = timm.list_models('vit*', pretrained = True)                      # Select all the vit models in timm\n",
    "print(\"Random selection of 10 models from the overall list of vit models:\\n\")\n",
    "print([vit_models[i] for i in torch.randperm(len(vit_models))[:10].tolist()]) # Select 10 random models from the list to display\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------Defining the model-----------------------------------#\n",
    "\n",
    "model_name = 'vit_tiny_patch16_224.augreg_in21k'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device = \", device)\n",
    "model = create_model(model_name, pretrained = True, num_classes = 10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65_4XPJudXBK"
   },
   "outputs": [],
   "source": [
    "loss_criterion = torch.nn.CrossEntropyLoss()                              # Cross-entropy loss is the loss criterion\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)   # SGD is the optimizer\n",
    "num_epochs = 2                                                            # Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1xfFMTZdXFz",
    "outputId": "441cd577-2c7b-4bb1-9e3a-89593abda98c"
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)           # get the inputs; data is a list of [inputs, labels]\n",
    "        optimizer.zero_grad()                                             # zero the parameter gradients for the fresh epoch\n",
    "\n",
    "        outputs = model(inputs)                                           # Feed forward step\n",
    "        loss = loss_criterion(outputs, labels)                            # Compute the loss \n",
    "        loss.backward()                                                   # Backward propagation\n",
    "        optimizer.step()                                                  # Update model parameter weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 0:                                                   # print every 10 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'vit_base_patch16_224'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device = \", device)\n",
    "model = create_model(model_name, pretrained = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Define the necessary image transformations--------#\n",
    "\n",
    "transform1 = torchvision.transforms.ToTensor()                   # Method that transforms the image format to PyTorch tensor\n",
    "transform2 = torchvision.transforms.Resize((224,224))            # Method that resizes a RGB image to (224,224,3) \n",
    "normalize_mean = (0.5, 0.5, 0.5)\n",
    "normalize_std = (0.5, 0.5, 0.5)        \n",
    "transform3 = torchvision.transforms.Normalize(normalize_mean, normalize_std)\n",
    "\n",
    "transforms = torchvision.transforms.Compose([transform1, transform2, transform3])   # Compose the two transforms into one master transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n",
    "imagenet_labels = dict(enumerate(open('ilsvrc2012_wordnet_lemmas.txt')))\n",
    "\n",
    "!wget https://github.com/ArunBaskaran/ViT-tutorial/blob/main/santorini.png?raw=true -O santorini.png\n",
    "#!wget https://github.com/ArunBaskaran/ViT-tutorial/blob/main/snowbird.jpg?raw=true -O snowbird.jpg\n",
    "img = PIL.Image.open('santorini.png') \n",
    "img_tensor = transforms(img).unsqueeze(0).to(device)\n",
    "\n",
    "img = np.array(img)\n",
    "img = resize(img, (224, 224, 3))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "output = model(img_tensor)                     # Obtain output from the model for this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Visualizing the test image and the model prediction-----------#\n",
    "print(f\"Inference Result: {imagenet_labels[int(torch.argmax(output))], int(torch.argmax(output))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Vision Transformers: Images as sequence of patches\n",
    "\n",
    "* Original shape of image: (224,224)\n",
    "\n",
    "* Divide them into patches of (16,16) -> 196 patches, each of shape (16,16)\n",
    "\n",
    "* Flatten each patch into a 1-D sequence -> 196 patches, each of size 256\n",
    "\n",
    "* Embed each patch into 768 dimensions using a linear layer -> 196 patches, each of size 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = model.patch_embed(img_tensor)                      # Converting input image into sequence of patches and embed them\n",
    "print(\"Shape of input image tensor: \", img_tensor.shape)                 \n",
    "print(\"Shape of sequence of patches: \", patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.suptitle(\"Division of input image into patches\", fontsize=18)\n",
    "img = np.asarray(img) #np.asarray(image.permute(1,2,0).numpy())\n",
    "for i in range(0, 196):                                      # Looping through each patch\n",
    "    x = i % 14                                               # Calculating the position of the patch in the 1st-dimension\n",
    "    y = i // 14                                              # Calculating the position of the patch in the 2nd-dimension\n",
    "    patch = img[y*16:(y+1)*16, x*16:(x+1)*16]                # Extracting the patch from the input image\n",
    "    ax = fig.add_subplot(14, 14, i+1)                        # Displaying the patch in the grid of plots\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.imshow(patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add position embeddings (pos emb)\n",
    "\n",
    "* Position embeddings are required to enable the model to learn which patches are in the local neighborhood, and which ones are in the global neighborhood.\n",
    "\n",
    "* This is especially relevant because the image is represented as a 1D-list of patches, and this representation does not inherently contain the 2D spatial awareness.\n",
    "\n",
    "* Position embedding is a vector of length 768 associated with each patch.\n",
    "\n",
    "* A well-learned position embedding should show similar embedding for patches that are close by in the 2D arrangement of patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed = model.pos_embed                                               # Extract the pos emb from the model\n",
    "print(\"Shape of the position embedding: \", pos_embed.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------Defining the cosine similarity function that will be used to compare the pos emb of the patches--------#\n",
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)                         \n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.suptitle(\"Visualization of position embedding similarities\", fontsize=24)\n",
    "\n",
    "for i in range(1, pos_embed.shape[1]):           # Looping through each patch's pos emb\n",
    "    # A similarity score is computed between this pos emb and every other pos emb\n",
    "    # So, 196 similarity scores are computed for each pos emb.\n",
    "    sim = torch.nn.functional.cosine_similarity(pos_embed[0, i:i+1], pos_embed[0, 1:], dim=1)\n",
    "    sim = sim.reshape((14, 14)).detach().cpu().numpy()\n",
    "    ax = fig.add_subplot(14, 14, i)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.imshow(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the input for the transformer\n",
    "\n",
    "* An additional patch (referred to as the class patch or the class token) is appended to the list of patches.\n",
    "\n",
    "* This patch will be fed forward along with the rest of the patches. After the transformer layers, the class patch will be used as input for the part of the model that does classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_input = torch.cat((model.cls_token, patches), dim=1)         # Add class patch to list of image patches\n",
    "transformer_input = transformer_input + pos_embed                        # Add pos emb\n",
    "print(\"Shape of the transformer input: \", transformer_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schematic of the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder\n",
    "\n",
    "<img src='https://github.com/hirotomusiker/schwert_colab_data_storage/blob/master/images/vit_demo/transformer_encoder.png?raw=true'>\n",
    "\n",
    "Image source: https://github.com/hirotomusiker/schwert_colab_data_storage/blob/master/images/vit_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input tensor to Transformer (z0): \", transformer_input.shape)\n",
    "x = transformer_input.clone()\n",
    "for i, blk in enumerate(model.blocks):\n",
    "    print(\"Entering the transformer encoder {}\".format(i))\n",
    "    x = blk(x)\n",
    "    print(f\"Output from transformer encoder {i}:\", x.shape)\n",
    "x = model.norm(x)\n",
    "transformer_output = x[:, 0]\n",
    "print(\"Output vector from Transformer (z12-0):\", transformer_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing attention layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformer Multi-head Attention block:\")\n",
    "block_no = 0\n",
    "if block_no == 0:\n",
    "    x = transformer_input.clone()\n",
    "    attention = model.blocks[block_no].attn\n",
    "    print(attention)\n",
    "    transformer_input_expanded = attention.qkv(x)[0]\n",
    "    print(\"input of the transformer encoder:\", transformer_input.shape)\n",
    "    print(\"expanded to: \", transformer_input_expanded.shape)\n",
    "else:\n",
    "    x = transformer_input.clone()\n",
    "    for n in range(block_no):\n",
    "        x = model.blocks[n](x)\n",
    "    attention = model.blocks[block_no].attn\n",
    "    print(attention)\n",
    "    transformer_input_expanded = attention.qkv(x)[0]\n",
    "    print(\"input of the transformer encoder:\", transformer_input.shape)\n",
    "    print(\"expanded to: \", transformer_input_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split qkv into mulitple q, k, and v vectors for multi-head attantion\n",
    "qkv = transformer_input_expanded.reshape(197, 3, 12, 64)  # (N=197, (qkv), H=12, D/H=64)\n",
    "print(\"split qkv : \", qkv.shape)\n",
    "q = qkv[:, 0].permute(1, 0, 2)  # (H=12, N=197, D/H=64) # Shape = 12 x 197 x 64\n",
    "k = qkv[:, 1].permute(1, 0, 2)  # shape = 12 x 197 x 64\n",
    "kT = k.permute(0, 2, 1)         # shape = 12 x 64 x 197\n",
    "print(\"transposed ks: \", kT.shape)\n",
    "\n",
    "attention_matrix = q @ kT       # matrix mul of (12 x 197 x 64) x (12 x 64 x 197) = (12 x 197 x 197)\n",
    "print(\"attention matrix: \", attention_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches        # Utility to draw shapes onto an existing image\n",
    "# Visualize attention matrix\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.suptitle(\"Visualization of attention scores from a particular patch\", fontsize=24)\n",
    "img = np.asarray(img)\n",
    "ax = fig.add_subplot(3, 4, 1)\n",
    "ax.imshow(img)\n",
    "n = 107\n",
    "x = n % 14\n",
    "y = n // 14\n",
    "rect = patches.Rectangle((x*16, y*16), 16, 16, linewidth=1, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "for i in range(11):  # visualize the n-th rows of attention matrices in the 0-7th heads\n",
    "    attn_heatmap = attention_matrix[i, n, 1:].reshape((14, 14)).detach().cpu().numpy()\n",
    "    ax = fig.add_subplot(3, 4, i+2)\n",
    "    ax.imshow(attn_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mean = torch.mean(attention_matrix, axis = 1)\n",
    "print(attn_mean.shape)\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.suptitle(\"Visualization of average attention scores from all patches\", fontsize=24)\n",
    "for i in range(12):\n",
    "    head_mean = attn_mean[i, :]\n",
    "    head_mean = head_mean[1:].reshape((14,14)).detach().cpu().numpy()\n",
    "    print(f'max and min of head {i+1}: {np.max(head_mean)}, {np.min(head_mean)}')\n",
    "    ax = fig.add_subplot(3, 4, i+1)\n",
    "    ax.imshow(head_mean)\n",
    "    \n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.suptitle(\"Visualization of class token\", fontsize=24)\n",
    "for i in range(12):\n",
    "    cls_token = attention_matrix[i, 1:, 0].reshape((14,14)).detach().cpu().numpy()\n",
    "    ax = fig.add_subplot(3, 4, i+1)\n",
    "    ax.imshow(cls_token)\n",
    "\n",
    "\"\"\"fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "cls_token_mean = torch.mean(attention_matrix[:,1:,0], axis = 0)\n",
    "cls_token_mean = cls_token_mean.reshape((14,14)).detach().cpu().numpy()\n",
    "ax.imshow(cls_token_mean)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0c_ablA67O2"
   },
   "source": [
    "## MLP Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the classification head: \", model.head)\n",
    "result = model.head(transformer_output)        # transformer_output: Output after passing input patch data through the 12 blocks\n",
    "result = torch.nn.functional.softmax(result, dim=1)  # softmax function to convert raw output to probabilities\n",
    "result_label_id = int(torch.argmax(result))    # Classification label (between 0 to 999)\n",
    "plt.plot(result.detach().cpu().numpy()[0])\n",
    "plt.title(\"Class probabilities\", size = 20)\n",
    "plt.xlabel(\"Class id\", size = 16)\n",
    "plt.ylabel('Probability', size = 16)\n",
    "print(\"Inference result : id = {}, label name = {}\".format(result_label_id, imagenet_labels[result_label_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Vision_Transformer_Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
